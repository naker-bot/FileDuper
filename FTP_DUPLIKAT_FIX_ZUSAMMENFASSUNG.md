# FTP-Duplikat-Fix Zusammenfassung (21. Oktober 2025)

## üîç HAUPTPROBLEM IDENTIFIZIERT

**Symptom:** "0 Duplikate gefunden" obwohl 7000+ Dateien gescannt wurden
**Root Cause:** ALLE FTP-Dateien bekamen Hash `"INVALID_FTP_URL"` und wurden als EINE Duplikat-Gruppe behandelt

## üêõ GEFUNDENE BUGS

### Bug #1: Hash-Storage filtert NICHT `INVALID_` Hashes
**Location:** `src/hashengine.cpp` Line 3107
**Problem:** 
```cpp
// ALT - Filtert NUR FTP_ aber nicht INVALID_
if (!hash.startsWith("FTP_") && !hash.startsWith("npv_")) {
    fileHashes[filePath] = hash;  // Speichert INVALID_FTP_URL!
}
```

**Fix:**
```cpp
// NEU - Filtert AUCH INVALID_ Hashes
if (!hash.startsWith("FTP_") && 
    !hash.startsWith("INVALID_") &&  // üî• KRITISCHER FIX
    !hash.startsWith("npv_")) {
    fileHashes[filePath] = hash;
}
```

**Ergebnis:** 
- VORHER: 6705 Dateien mit Hash `"INVALID_FTP_URL"` ‚Üí 1 Gruppe mit 6704 "Duplikaten"
- NACHHER: `INVALID_` Hashes werden NICHT gespeichert ‚Üí Nur echte Duplikate

### Bug #2: FTP-URLs haben `///` im Pfad
**Location:** Scanner FTP-File-Collection
**Problem:**
```
Pfad: "F_Mukke///share/Jan/heiner/Musik/abc_die_katze1.rg"
URL:  "ftp://192.168.50.224:21F_Mukke///share/Jan/..."
```

**Status:** ‚ö†Ô∏è **NOCH NICHT VOLLST√ÑNDIG GEL√ñST**
- `///`-Cleaning existiert in `scanner.cpp` Line 2130
- Aber FTP-Files kommen von `MainWindow::onFtpFileListReceived`
- Cleaning muss DORT angewendet werden!

### Bug #3: Host-Variable manchmal leer
**Location:** `scanner.cpp` Line 2112
**Problem:** `QString host = ftpUrl.host();` gibt leeren String wenn URL ung√ºltig
**Ergebnis:** URLs wie `"ftp://:21/share/..."` ohne Hostname

**Fix ben√∂tigt:** Host-Validierung vor URL-Konstruktion

## ‚úÖ IMPLEMENTIERTE FIXES

1. **INVALID_ Hash-Filter** (hashengine.cpp:3107) ‚úÖ KOMPILIERT
2. **URL-Slash-Fix** (scanner.cpp:2161) ‚úÖ IM CODE
3. **Leere Hash-Filterung** (hashengine.cpp:2883) ‚úÖ FUNKTIONIERT

## ‚ö†Ô∏è NOCH ZU FIXEN

1. **`///` Cleaning in MainWindow** - FTP-Files kommen von MainWindow nicht Scanner
2. **Host-Validierung** - Prevent empty host in FTP URLs
3. **FTP-URL-Format** - Ensure `ftp://host:port/path` format

## üìä TEST-ERGEBNISSE

### Aktueller Status (21. Okt 23:54)
```
Binary: ./FileDuper (PID 700605)
Log: /tmp/fd_invalid_filter_fix.log
```

**Erwartetes Verhalten nach Fix:**
- `INVALID_FTP_URL` Hashes werden NICHT gespeichert
- Nur g√ºltige FTP-Hashes landen in `fileHashes`
- Echte Duplikate basierend auf Datei-Content, nicht Fehlern

### Test-Anweisung
```bash
1. Starte FileDuper GUI (PID 700605 l√§uft bereits)
2. W√§hle FTP-Verzeichnisse aus (/share/Jan/heiner/Musik, etc.)
3. Starte Duplikat-Scan
4. Pr√ºfe Log:
   grep "INVALID_" /tmp/fd_invalid_filter_fix.log
   # Sollte KEINE "Hash gespeichert: ... ‚Üí INVALID_" Zeilen zeigen
```

## üîß N√ÑCHSTE SCHRITTE

1. **User scannt nochmal** mit neuer Binary
2. **Pr√ºfe Log** auf:
   - Keine `INVALID_` Hashes in Hash-Storage
   - G√ºltige FTP-URLs: `ftp://192.168.50.224:21/share/...`
   - Echte Duplikat-Gruppen basierend auf Content
3. **Fix `///` Problem** wenn immer noch vorhanden

## üìù LESSONS LEARNED

- **Hash-Filter m√ºssen vollst√§ndig sein**: Nicht nur `FTP_` sondern auch `INVALID_`, `ERROR_`, etc.
- **FTP-File-Collection hat 2 Wege**: Scanner UND MainWindow - beide m√ºssen Pfade bereinigen
- **URL-Validierung KRITISCH**: Leere Hosts f√ºhren zu ung√ºltigen URLs

---

**Status:** ‚úÖ INVALID-Filter kompiliert und deployed
**N√§chster Test:** User-Scan mit neuer Binary
